# Kavak Demo - Bandits + LLM Evaluators

MVP demo de agentes conversacionales con aprendizaje online (multi-armed bandits) y evaluaciÃ³n automÃ¡tica mediante LLM para optimizar atenciÃ³n al cliente en Kavak.

## ğŸ¯ Objetivo

Demostrar cÃ³mo los agentes LLM pueden:
1. **Personalizar** respuestas usando plantillas optimizadas por contexto
2. **Aprender online** quÃ© plantillas funcionan mejor para cada segmento/issue
3. **Evaluar automÃ¡ticamente** la calidad de las respuestas con mÃ©tricas objetivas
4. **Priorizar** outreach proactivo a clientes de alto valor

## ğŸ—ï¸ Arquitectura

```
PersonaForge â†’ State Builder â†’ Policy (Bandit) â†’ Template Factory
                                    â†“
                            Responder/Outreach Agent
                                    â†“
                              Judge (Evaluator)
                                    â†“
                            Reward â†’ Update Policy
```

### Componentes Principales

- **PersonaForge**: Genera dataset sintÃ©tico de clientes con perfiles diversos
- **State Builder**: Empaqueta features del cliente en contexto estructurado
- **Template Factory**: 5 plantillas parametrizadas (empÃ¡tico, tÃ©cnico, cupÃ³n, escalaciÃ³n, seguimiento)
- **Policy Learner**: Thompson Sampling o Îµ-greedy para selecciÃ³n de plantillas
- **Responder Agent**: Genera respuestas para clientes vocales (con mensaje inicial)
- **Outreach Agent**: Genera mensajes proactivos para clientes no-vocales
- **Judge**: EvalÃºa mensajes con LLM â†’ {NPS, Engagement, Churn, AspectSentiment}
- **Prioritizer**: Rankea clientes para outreach basado en valor/riesgo
- **Safety**: Filtros de PII, tÃ©rminos prohibidos, tono inapropiado

## ğŸš€ Quick Start

### 1. InstalaciÃ³n

```bash
# Clonar repo
git clone <repo-url>
cd hackathon-kavak-openai

# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
```

### 2. ConfiguraciÃ³n

```bash
# Copiar template de variables de entorno
cp .env.example .env

# Editar .env y agregar tu API key
OPENAI_API_KEY=sk-...
```

### 3. Ejecutar Demo

```bash
# Iniciar Streamlit
streamlit run app/streamlit_app.py
```

La app abrirÃ¡ en `http://localhost:8501`

## ğŸ“Š Uso

### Workflow BÃ¡sico

1. **Generar Dataset**: Click en "ğŸ² Generar Dataset" (sidebar)
   - Configura nÃºmero de clientes (default: 100)
   - Genera perfiles sintÃ©ticos con segmentos, issues, NPS, etc.

2. **Ejecutar IteraciÃ³n**: Click en "â–¶ï¸ Ejecutar IteraciÃ³n"
   - Procesa clientes vocales (respuestas)
   - Genera outreach proactivo (top-N no-vocales)
   - EvalÃºa cada mensaje con Judge
   - Actualiza polÃ­tica del bandit

3. **Revisar Resultados** en los tabs:
   - **Dashboard**: KPIs, grÃ¡ficos de reward, distribuciÃ³n de plantillas
   - **Conversaciones**: Mensajes generados para vocales + scores
   - **Outreach**: Mensajes proactivos + scores
   - **MÃ©tricas**: Desglose por segmento/plantilla/issue
   - **Recomendaciones**: Insights automÃ¡ticos y mejores combinaciones

### ConfiguraciÃ³n Avanzada

**Sidebar options:**
- NÃºmero de clientes: 10-500
- Algoritmo: Thompson Sampling o Îµ-greedy
- Top N outreach: 5-100
- Modelos: gpt-4o, gpt-4o-mini, gpt-3.5-turbo

**Iteraciones mÃºltiples:**
- Ejecuta 2-3 iteraciones para ver aprendizaje online
- El bandit converge hacia mejores plantillas por contexto

## ğŸ“ Estructura del Proyecto

```
hackathon-kavak-openai/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ streamlit_app.py          # UI principal
â”‚   â”œâ”€â”€ models.py                 # Pydantic models
â”‚   â”œâ”€â”€ templates.py              # Definiciones de plantillas
â”‚   â”œâ”€â”€ scoring.py                # CÃ¡lculo de reward
â”‚   â”œâ”€â”€ factories/
â”‚   â”‚   â”œâ”€â”€ persona_forge.py      # Generador de clientes
â”‚   â”‚   â”œâ”€â”€ state_builder.py      # Constructor de contexto
â”‚   â”‚   â”œâ”€â”€ template_factory.py   # Relleno de plantillas
â”‚   â”‚   â”œâ”€â”€ responders.py         # Agents (Responder/Outreach)
â”‚   â”‚   â”œâ”€â”€ judge.py              # Evaluador LLM
â”‚   â”‚   â”œâ”€â”€ policy_learner.py     # Bandits (Thompson/Îµ-greedy)
â”‚   â”‚   â”œâ”€â”€ prioritizer.py        # Ranker de outreach
â”‚   â”‚   â”œâ”€â”€ metrics.py            # AgregaciÃ³n de KPIs
â”‚   â”‚   â””â”€â”€ safety.py             # Filtros de contenido
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ AGENTS.md                     # EspecificaciÃ³n tÃ©cnica completa
```

## ğŸ¯ MÃ©tricas y KPIs

### Reward Function

```python
R = 0.6 * NPS_expected + 0.3 * (Engagement * 10) - 0.3 * (Churn * 10)
Normalizado a [0, 1]
```

### Judge Scores

- **NPS_expected** (0-10): NPS anticipado del cliente tras recibir el mensaje
- **EngagementProb** (0-1): Probabilidad de respuesta/acciÃ³n del cliente
- **ChurnProb** (0-1): Probabilidad de abandono/churn
- **AspectSentiment** (-1 a 1): Sentimiento por aspecto (finanzas, mecÃ¡nica, logÃ­stica, atenciÃ³n)
- **Rationale** (â‰¤280 chars): ExplicaciÃ³n de la evaluaciÃ³n

## ğŸ§ª Testing RÃ¡pido

Checklist de 15 minutos:

- [ ] Judge devuelve JSON vÃ¡lido con 5 campos
- [ ] Reward en rango [0, 1]
- [ ] Bandit actualiza distribuciÃ³n tras rewards
- [ ] Baseline vs policy: diferencia visible en 2Âª-3Âª iteraciÃ³n
- [ ] Safety bloquea tÃ©rminos prohibidos
- [ ] Streamlit refresca sin reiniciar sesiÃ³n

## ğŸ”’ Safety & Compliance

**Guardrails automÃ¡ticos:**
- DetecciÃ³n de PII (RFC, tarjetas, SSN)
- TÃ©rminos prohibidos (garantÃ­as absolutas, claims legales)
- LÃ­mites de compensaciÃ³n (mÃ¡x 20%)
- ValidaciÃ³n de tono (no culpar cliente, incluir next step)

**Fallbacks:**
- Mensajes seguros si Judge falla
- Mensaje genÃ©rico si Safety bloquea contenido

## ğŸš¦ Roadmap

### Fase 1 (Actual - MVP)
- [x] Dataset sintÃ©tico con PersonaForge
- [x] 5 plantillas base
- [x] Thompson Sampling + Îµ-greedy
- [x] Judge con OpenAI
- [x] UI Streamlit con 5 tabs
- [x] Safety bÃ¡sico

### Fase 2 (1-2 semanas)
- [ ] Conectar CRM real (histÃ³ricos, tickets)
- [ ] Micro-acciones (cupÃ³n %, SLA especÃ­fico)
- [ ] Contextual bandits (LinUCB con features)
- [ ] A/B testing framework
- [ ] Aspect-based sentiment (ABS) granular

### Fase 3 (1 mes)
- [ ] Deployment productivo
- [ ] Monitoreo real-time
- [ ] Human-in-the-loop para casos edge
- [ ] Expansion de plantillas automÃ¡tica

## ğŸ› ï¸ Troubleshooting

**Error: "OpenAI API key required"**
- Verifica que `.env` tenga `OPENAI_API_KEY=sk-...`
- O ingresa la key en el sidebar de la UI

**Error: "Judge evaluation failed"**
- Revisa quota/lÃ­mites de la API
- El sistema usa fallback score por defecto

**Mensajes bloqueados por Safety**
- Revisa logs en terminal
- Ajusta `PROHIBITED_TERMS` en `app/factories/safety.py`

**Bandit no aprende (reward flat)**
- Ejecuta mÃ¡s iteraciones (mÃ­nimo 2-3)
- Aumenta nÃºmero de clientes
- Verifica que Judge estÃ© devolviendo scores diversos

## ğŸ“š Referencias

- [AGENTS.md](./AGENTS.md) - EspecificaciÃ³n tÃ©cnica completa
- [OpenAI Agents API](https://platform.openai.com/docs/guides/agents)
- [Thompson Sampling](https://en.wikipedia.org/wiki/Thompson_sampling)
- [Multi-Armed Bandits](https://en.wikipedia.org/wiki/Multi-armed_bandit)

## ğŸ‘¥ Equipo

Kavak Hackathon - OpenAI Agents Track

## ğŸ“„ Licencia

MIT
